/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as environments from "../../../../environments";
import * as core from "../../../../core";
import * as Letta from "../../../index";
import { Messages } from "../resources/messages/client/Client";
export declare namespace Batches {
    interface Options {
        environment?: core.Supplier<environments.LettaEnvironment | string>;
        /** Specify a custom URL to connect the client to. */
        baseUrl?: core.Supplier<string>;
        token?: core.Supplier<string | undefined>;
        /** Override the X-Project header */
        project?: core.Supplier<string | undefined>;
        fetcher?: core.FetchFunction;
    }
    interface RequestOptions {
        /** The maximum time to wait for a response in seconds. */
        timeoutInSeconds?: number;
        /** The number of times to retry the request. Defaults to 2. */
        maxRetries?: number;
        /** A hook to abort the request. */
        abortSignal?: AbortSignal;
        /** Override the X-Project header */
        project?: string | undefined;
        /** Additional headers to include in the request. */
        headers?: Record<string, string>;
    }
}
export declare class Batches {
    protected readonly _options: Batches.Options;
    protected _messages: Messages | undefined;
    constructor(_options?: Batches.Options);
    get messages(): Messages;
    /**
     * List all batch runs.
     *
     * @param {Letta.BatchesListRequest} request
     * @param {Batches.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Letta.UnprocessableEntityError}
     *
     * @example
     *     await client.batches.list({
     *         before: "before",
     *         after: "after",
     *         limit: 1,
     *         order: "asc",
     *         orderBy: "created_at"
     *     })
     */
    list(request?: Letta.BatchesListRequest, requestOptions?: Batches.RequestOptions): core.HttpResponsePromise<Letta.BatchJob[]>;
    private __list;
    /**
     * Submit a batch of agent runs for asynchronous processing.
     *
     * Creates a job that will fan out messages to all listed agents and process them in parallel.
     * The request will be rejected if it exceeds 256MB.
     *
     * @param {Letta.CreateBatch} request
     * @param {Batches.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Letta.UnprocessableEntityError}
     *
     * @example
     *     await client.batches.create({
     *         requests: [{
     *                 messages: [{
     *                         role: "user",
     *                         content: [{
     *                                 type: "text",
     *                                 text: "text"
     *                             }]
     *                     }],
     *                 agentId: "agent_id"
     *             }]
     *     })
     */
    create(request: Letta.CreateBatch, requestOptions?: Batches.RequestOptions): core.HttpResponsePromise<Letta.BatchJob>;
    private __create;
    /**
     * Retrieve the status and details of a batch run.
     *
     * @param {string} batchId
     * @param {Batches.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Letta.UnprocessableEntityError}
     *
     * @example
     *     await client.batches.retrieve("batch_id")
     */
    retrieve(batchId: string, requestOptions?: Batches.RequestOptions): core.HttpResponsePromise<Letta.BatchJob>;
    private __retrieve;
    /**
     * Cancel a batch run.
     *
     * @param {string} batchId
     * @param {Batches.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link Letta.UnprocessableEntityError}
     *
     * @example
     *     await client.batches.cancel("batch_id")
     */
    cancel(batchId: string, requestOptions?: Batches.RequestOptions): core.HttpResponsePromise<unknown>;
    private __cancel;
    protected _getCustomAuthorizationHeaders(): Promise<{
        Authorization: string;
    }>;
}
