/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as Letta from "../../../../index";
/**
 * @example
 *     {
 *         model: "model",
 *         messages: [{
 *                 content: "content",
 *                 role: "developer"
 *             }]
 *     }
 */
export interface ChatCompletionRequest {
    /** ID of the model to use */
    model: string;
    /** Messages comprising the conversation so far */
    messages: Letta.ChatCompletionRequestMessagesItem[];
    /** Sampling temperature */
    temperature?: number;
    /** Nucleus sampling parameter */
    topP?: number;
    /** Number of chat completion choices to generate */
    n?: number;
    /** Whether to stream back partial progress */
    stream?: boolean;
    /** Sequences where the API will stop generating */
    stop?: Letta.ChatCompletionRequestStop;
    /** Maximum number of tokens to generate */
    maxTokens?: number;
    /** Presence penalty */
    presencePenalty?: number;
    /** Frequency penalty */
    frequencyPenalty?: number;
    /** A unique identifier representing your end-user */
    user?: string;
}
